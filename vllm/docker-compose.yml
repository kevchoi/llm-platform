services:
  vllm-server:
    image: vllm/vllm-openai:v0.10.2 # Last version that supports V0 engine
    command:
      - '--model'
      - 'meta-llama/Llama-3.2-1B'
    volumes:
      - ${HOME}/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - VLLM_USE_V1=0 # vLLM v1 does not support T4 GPUs
    ports:
      - "8000:8000"
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
  dcgm-exporter:
    image: nvidia/dcgm-exporter:4.4.1-4.5.2-ubuntu22.04
    ports:
      - "9400:9400"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    cap_add:
      - SYS_ADMIN
  node-exporter:
    image: prom/node-exporter:latest
    command:
      - '--path.rootfs=/host'
    volumes:
      - /:/host:ro,rslave # exposes the root filesystem; instead of just /proc and /sys
    network_mode: host
    pid: host
    ports:
      - "9100:9100"