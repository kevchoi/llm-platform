# Provisioning an EKS cluster (AWS)

https://developer.hashicorp.com/terraform/tutorials/kubernetes/eks

## Prerequisites

- Terraform
- AWS CLI
- kubectl

## Configuration

This project uses an S3 bucket to store the Terraform state remotely, which is a recommended practice for collaboration and state management.

1.  **Create an S3 bucket for Terraform state:**
    You need an S3 bucket to store the `terraform.tfstate` file. Replace `your-terraform-state-bucket` with a unique name for your S3 bucket.

    ```sh
    aws s3api create-bucket --bucket your-terraform-state-bucket --region us-east-1
    ```

    It is recommended to enable versioning and encryption on this bucket to protect your Terraform state.

2.  **Update backend configuration:**
    In the `infra/terraform/environments/staging/backend.tfvars` file, update the bucket name with the one you just created.

    ```terraform:infra/terraform/environments/staging/backend.tfvars
    bucket = "your-terraform-state-bucket"
    ```

## Deployment

The following commands should be run from the `staging` environment directory.

1.  **Navigate to the staging directory:**
    ```sh
    cd infra/terraform/environments/staging
    ```

2.  **Initialize Terraform:**
    This command initializes Terraform and configures the S3 backend.
    ```sh
    terraform init -backend-config=backend.tfvars
    ```

3.  **Plan the deployment:**
    This command shows you what resources will be created.
    ```sh
    terraform plan
    ```

4.  **Apply the configuration:**
    This command will create the AWS resources.
    ```sh
    terraform apply
    ```

    **Note on ArgoCD Application:**

    The deployment includes an ArgoCD application managed via a `kubernetes_manifest` resource in Terraform. This resource requires API access to the Kubernetes cluster during `terraform plan`, which means the cluster must be running before this resource can be applied.

    Because of this, the initial deployment requires two `terraform apply` steps:

    1.  **First `apply`:** In `infra/terraform/environments/staging/main.tf`, comment out the `kubernetes_manifest` resource for the `argocd_app`.
        ```hcl
        # resource "kubernetes_manifest" "argocd_app" {
        #   ...
        # }
        ```
        Then run `terraform apply`. This will provision the EKS cluster and all other resources.

    2.  **Second `apply`:** Uncomment the `kubernetes_manifest.argocd_app` resource and run `terraform apply` again. This will deploy the ArgoCD application into the now-running cluster.

## Post-deployment

Once the EKS cluster is created, you need to configure `kubectl` to connect to it.

1.  **Update your kubeconfig:**
    You can get the cluster name from the Terraform output. Then run the following AWS CLI command:
    ```sh
    aws eks update-kubeconfig --name llm-staging --region us-east-1
    ```

2.  **Verify connection:**
    ```sh
    kubectl get nodes
    ```

## Accessing ArgoCD

ArgoCD is installed via a Helm chart as part of the EKS module. It is a declarative, GitOps continuous delivery tool for Kubernetes.

1.  **Access the ArgoCD UI:**
    The ArgoCD API server is not exposed to the internet by default. You can access it by port-forwarding the service to your local machine.

    ```sh
    kubectl port-forward svc/argocd-server -n argocd 8080:443
    ```
    You can then access the UI by navigating to `https://localhost:8080` in your web browser.

2.  **Log in to ArgoCD:**
    The default username for ArgoCD is `admin`. The initial password is automatically generated and stored in a Kubernetes secret. You can retrieve it with the following command:

    ```sh
    kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d
    ```
    You can use this password to log in to the UI. It is recommended to change this password after your first login.

## Monitoring with Prometheus and Grafana

The `kube-prometheus-stack` is deployed as an ArgoCD application to provide monitoring capabilities for the Kubernetes cluster. This stack includes Prometheus for collecting metrics and Grafana for visualizing them.

1.  **Access the Prometheus UI:**
    The Prometheus server is not exposed publicly. Use `port-forward` to access it.

    ```sh
    kubectl port-forward svc/kube-prometheus-stack-prometheus -n monitoring 9090:9090
    ```
    You can then access the UI by navigating to `http://localhost:9090` in your web browser.

2.  **Access the Grafana UI:**
    Grafana is also not exposed publicly. Use `port-forward` to access it.

    ```sh
    kubectl port-forward svc/kube-prometheus-stack-grafana -n monitoring 3000:80
    ```
    You can then access the UI by navigating to `http://localhost:3000` in your web browser. The default credentials are `admin` / `prom-operator`.

## NVIDIA GPU Operator

The NVIDIA GPU Operator is installed as an ArgoCD application, which is required to provision and manage GPU resources on the EKS cluster.

## Security Considerations

-   **EKS Public Endpoint**: The EKS cluster is configured with a public API endpoint (`endpoint_public_access = true` in `infra/terraform/environments/staging/main.tf`). For a production environment, you should consider setting this to `false` and accessing the cluster via a bastion host or VPN for enhanced security.

-   **Cluster Creator Admin Permissions**: The `enable_cluster_creator_admin_permissions` flag is set to `true`. This gives the IAM user/role that creates the cluster `system:masters` permissions in Kubernetes RBAC. While convenient for initial setup, for production it is better to manage access control through Kubernetes RBAC roles and bindings explicitly.