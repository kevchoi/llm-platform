{
    "model": "gpt-5-nano-2025-08-07",
    "messages": [
      {
        "role": "system",
        "content": "You are a concise summarization assistant."
      },
      {
        "role": "user",
        "content": "Summarize the following passage in a few sentences:\n\nSeven principles\nProcess tokens faster.\nGenerate fewer tokens.\nUse fewer input tokens.\nMake fewer requests.\nParallelize.\nMake your users wait less.\nDon't default to an LLM.\nProcess tokens faster\nInference speed is probably the first thing that comes to mind when addressing latency (but as you'll see soon, it's far from the only one). This refers to the actual rate at which the LLM processes tokens, and is often measured in TPM (tokens per minute) or TPS (tokens per second).\n\nThe main factor that influences inference speed is model size \u2013 smaller models usually run faster (and cheaper), and when used correctly can even outperform larger models. To maintain high quality performance with smaller models you can explore:\n\nusing a longer, more detailed prompt,\nadding (more) few-shot examples, or\nfine-tuning / distillation.\nYou can also employ inference optimizations like our \nPredicted outputs\n feature. Predicted outputs let you significantly reduce latency of a generation when you know most of the output ahead of time, such as code editing tasks. By giving the model a prediction, the LLM can focus more on the actual changes, and less on the content that will remain the same.\n\nDeep dive\nCompute capacity & additional inference optimizations\nGenerate fewer tokens\nGenerating tokens is almost always the highest latency step when using an LLM: as a general heuristic, cutting 50% of your output tokens may cut ~50% your latency. The way you reduce your output size will depend on output type:\n\nIf you're generating natural language, simply asking the model to be more concise (\"under 20 words\" or \"be very brief\") may help. You can also use few shot examples and/or fine-tuning to teach the model shorter responses.\n\nIf you're generating structured output, try to minimize your output syntax where possible: shorten function names, omit named arguments, coalesce parameters, etc.\n\nFinally, while not common, you can also use max_tokens or stop_tokens to end your generation early.\n\nAlways remember: an output token cut is a (milli)second earned!\n\nUse fewer input tokens\nWhile reducing the number of input tokens does result in lower latency, this is not usually a significant factor \u2013 cutting 50% of your prompt may only result in a 1-5% latency improvement. Unless you're working with truly massive context sizes (documents, images), you may want to spend your efforts elsewhere.\n\nThat being said, if you are working with massive contexts (or you're set on squeezing every last bit of performance and you've exhausted all other options) you can use the following techniques to reduce your input tokens:\n\nFine-tuning the model, to replace the need for lengthy instructions / examples.\nFiltering context input, like pruning RAG results, cleaning HTML, etc.\nMaximize shared prompt prefix, by putting dynamic portions (e.g. RAG results, history, etc) later in the prompt. This makes your request more KV cache-friendly (which most LLM providers use) and means fewer input tokens are processed on each request.\nCheck out our docs to learn more about how prompt caching works.\n\nMake fewer requests\nEach time you make a request you incur some round-trip latency \u2013 this can start to add up.\n\nIf you have sequential steps for the LLM to perform, instead of firing off one request per step consider putting them in a single prompt and getting them all in a single response. You'll avoid the additional round-trip latency, and potentially also reduce complexity of processing multiple responses.\n\nAn approach to doing this is by collecting your steps in an enumerated list in the combined prompt, and then requesting the model to return the results in named fields in a JSON. This way you can easily parse out and reference each result!\n\nParallelize\nParallelization can be very powerful when performing multiple steps with an LLM.\n\nIf the steps are not strictly sequential, you can split them out into parallel calls. Two shirts take just as long to dry as one.\n\nIf the steps are strictly sequential, however, you might still be able to leverage speculative execution. This is particularly effective for classification steps where one outcome is more likely than the others (e.g. moderation).\n\nStart step 1 & step 2 simultaneously (e.g. input moderation & story generation)\nVerify the result of step 1\nIf result was not the expected, cancel step 2 (and retry if necessary)\nIf your guess for step 1 is right, then you essentially got to run it with zero added latency!\n\nMake your users wait less\nThere's a huge difference between waiting and watching progress happen \u2013 make sure your users experience the latter. Here are a few techniques:\n\nStreaming: The single most effective approach, as it cuts the waiting time to a second or less. (ChatGPT would feel pretty different if you saw nothing until each response was done.)\nChunking: If your output needs further processing before being shown to the user (moderation, translation) consider processing it in chunks instead of all at once. Do this by streaming to your backend, then sending processed chunks to your frontend.\nShow your steps: If you're taking multiple steps or using tools, surface this to the user. The more real progress you can show, the better.\nLoading states: Spinners and progress bars go a long way.\nNote that while showing your steps & having loading states have a mostly psychological effect, streaming & chunking genuinely do reduce overall latency once you consider the app + user system: the user will finish reading a response sooner.\n\nDon't default to an LLM\nLLMs are extremely powerful and versatile, and are therefore sometimes used in cases where a faster classical method would be more appropriate. Identifying such cases may allow you to cut your latency significantly. Consider the following examples:\n\nHard-coding: If your output is highly constrained, you may not need an LLM to generate it. Action confirmations, refusal messages, and requests for standard input are all great candidates to be hard-coded. (You can even use the age-old method of coming up with a few variations for each.)\nPre-computing: If your input is constrained (e.g. category selection) you can generate multiple responses in advance, and just make sure you never show the same one to a user twice.\nLeveraging UI: Summarized metrics, reports, or search results are sometimes better conveyed with classical, bespoke UI components rather than LLM-generated text.\nTraditional optimization techniques: An LLM application is still an application; binary search, caching, hash maps, and runtime complexity are all still useful in a world of LLMs."
  }
  ],
  "max_tokens": 1024
}