services:
  tgi-server:
    image: ghcr.io/huggingface/text-generation-inference:3.3.4
    command:
      - '--model-id meta-llama/Llama-3.2-1B'
    volumes:
      - ./data:/data
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    ports:
      - "8080:80"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
  dcgm-exporter:
    image: nvidia/dcgm-exporter:4.4.1-4.5.2-ubuntu22.04
    ports:
      - "9400:9400"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    cap_add:
      - SYS_ADMIN
  node-exporter:
    image: prom/node-exporter:latest
    command:
      - '--path.rootfs=/host'
    volumes:
      - /:/host:ro,rslave # exposes the root filesystem; instead of just /proc and /sys
    network_mode: host
    pid: host
    ports:
      - "9100:9100"